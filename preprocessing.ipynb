{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e7a39e",
   "metadata": {},
   "source": [
    "### RAHUL SAI SAMINENI\n",
    "### 582 - IR PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82bef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependency libraries\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deba6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting english stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Initializing Porter Stemmer object\n",
    "st = PorterStemmer()\n",
    "\n",
    "# Initializing regex to remove words with one or two characters length\n",
    "# shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "# folder to store pickel files\n",
    "pickle_folder = \"./Pickle/\"\n",
    "os.makedirs(pickle_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e7450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_folder = \"./PagesFetched/\"\n",
    "filenames = os.listdir(pages_folder)\n",
    "\n",
    "# list to store filenames of all stored crawled webpages\n",
    "files = []\n",
    "\n",
    "for name in filenames:\n",
    "    files.append(name)\n",
    "\n",
    "# len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5696bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter tags that are visible on webpage i.e. excluding style, script, meta, etc. tags\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'meta', '[document]']:\n",
    "        return False\n",
    "    elif isinstance(element, Comment):                     # check if element is html comment\n",
    "        return False\n",
    "    elif re.match(r\"[\\s\\r\\n]+\",str(element)):              #  to eliminate remaining extra white spaces and new lines\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59aaf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract only the visible text from the html code of each webpage\n",
    "\n",
    "def get_text_from_code(page):\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    text_in_page = soup.find_all(text=True)                # return all text in page\n",
    "    visible_text = filter(tag_visible, text_in_page)       # return only visible text\n",
    "    return \" \".join(term.strip() for term in visible_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9f1915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulsaisamineni/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# dict to create inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# dict to store tokens in each web page\n",
    "webpage_tokens = {}\n",
    "\n",
    "for file in files:\n",
    "    web_page = open(pages_folder + file, \"r\", encoding=\"utf-8\")\n",
    "    code = web_page.read()\n",
    "    text = get_text_from_code(code)                     # get all text actually visible on web page\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-z]+', ' ', text)                 # remove all punctuations and digits\n",
    "    \n",
    "    tokens = text.split()\n",
    "\n",
    "    # removing stop words and stemming each token while only accepting stemmed tokens with length greater than 2 \n",
    "    clean_stem_tokens = [\n",
    "        st.stem(token) for token in tokens \n",
    "        if (token not in stop_words and st.stem(token) not in stop_words) and len(st.stem(token))>2\n",
    "    ]\n",
    "    \n",
    "    webpage_tokens[file] = clean_stem_tokens                        # add tokens in web page to dict \n",
    "    \n",
    "    for token in clean_stem_tokens:\n",
    "        \n",
    "        freq = inverted_index.setdefault(token,{}).get(file,0)      # get frequency of token and set to 0 if token not in dict\n",
    "            \n",
    "        inverted_index.setdefault(token,{})[file] = freq  + 1       # add 1 to frequency of token in current webpage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3340a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling inverted index and tokens\n",
    "\n",
    "with open(pickle_folder + '6k_inverted_index.pickle', 'wb') as f:\n",
    "    pickle.dump(inverted_index,f)\n",
    "    \n",
    "with open(pickle_folder + '6k_webpages_tokens.pickle', 'wb') as f:\n",
    "    pickle.dump(webpage_tokens,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69135741",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
