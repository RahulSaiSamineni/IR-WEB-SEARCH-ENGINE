{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "008e948e",
   "metadata": {},
   "source": [
    "### Rahul Sai Samineni\n",
    "### IR-582-Final project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c442bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependency libraries\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e703f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of webpages indexed\n",
    "N = 6001\n",
    "\n",
    "# extracting english stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Initializing Porter Stemmer object\n",
    "st = PorterStemmer()\n",
    "\n",
    "# folder to store pickel files\n",
    "pickle_folder = \"./Pickle/\"\n",
    "os.makedirs(pickle_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d500ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpages_idf = {}\n",
    "max_freq = {}\n",
    "webpages_tf_idf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91c4c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unloading all the pickle files\n",
    "with open(pickle_folder + \"6k_inverted_index.pickle\", \"rb\") as f:\n",
    "    inverted_index = pickle.load(f)\n",
    "\n",
    "with open(pickle_folder + \"6k_webpages_tokens.pickle\", \"rb\") as f:\n",
    "    webpages_tokens = pickle.load(f)\n",
    "\n",
    "with open(pickle_folder + \"6k_pages_crawled.pickle\", \"rb\") as f:\n",
    "    urls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ce6fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for computing idf of each token in the collection of webpages\n",
    "def calc_idf(inverted_index):\n",
    "    idf = {}\n",
    "    for key in inverted_index.keys():\n",
    "        df = len(inverted_index[key].keys())\n",
    "        # calculating IDF for a token\n",
    "        idf[key] = math.log2(N / df)\n",
    "    return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e380311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tfidf(inverted_index):\n",
    "    # making a temporary copy of the inverted index\n",
    "    tf_idf = copy.deepcopy(inverted_index)\n",
    "    for token in tf_idf:\n",
    "        for page in tf_idf[token]:\n",
    "            # calculating TF for the webpage\n",
    "            tf = tf_idf[token][page] / max_freq[page]\n",
    "            # calculating TF-IDF for token\n",
    "            tf_idf[token][page] = tf * webpages_idf[token]\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2e32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate document vector length for a particular webpage\n",
    "def calc_doc_len(doc, doc_tokens):\n",
    "    doc_len = 0\n",
    "    for token in set(doc_tokens):\n",
    "        # adding square of weight of token to document vector length\n",
    "        doc_len += webpages_tf_idf[token][doc] ** 2\n",
    "    doc_len = math.sqrt(doc_len)\n",
    "    return doc_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b797fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate document vector lengths for all fetched webpages\n",
    "def doc_len_pages(list_of_tokens):\n",
    "    doc_lens = {}\n",
    "    for page in list_of_tokens:\n",
    "        doc_lens[page] = calc_doc_len(page, list_of_tokens[page])\n",
    "    return doc_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4a092fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity\n",
    "def calc_cos_sim_scores(query, doc_lens):\n",
    "    similarity_scores = {}\n",
    "    query_len = 0\n",
    "    query_weights = {}\n",
    "    # create count dictionary of the query\n",
    "    query_dict = Counter(query)\n",
    "\n",
    "    for token in query_dict.keys():\n",
    "        # calculate query token TF\n",
    "        token_tf = query_dict[token] / query_dict.most_common(1)[0][1]\n",
    "        # calculate query token TF-IDF of token\n",
    "        query_weights[token] = token_tf * webpages_idf.get(token, 0)\n",
    "        # add square of token weight to query vector length\n",
    "        query_len += query_weights[token] ** 2\n",
    "\n",
    "    query_len = math.sqrt(query_len)\n",
    "\n",
    "    for token in query:\n",
    "        token_weight = query_weights.get(token)\n",
    "\n",
    "        if token_weight:\n",
    "            # calculating inner product between query and webpages\n",
    "            for page in webpages_tf_idf[token].keys():\n",
    "                similarity_scores[page] = similarity_scores.get(page, 0) + (\n",
    "                    webpages_tf_idf[token][page] * token_weight\n",
    "                )\n",
    "\n",
    "    # dividing inner product by product of doc lens of query and webpage\n",
    "    for page in similarity_scores:\n",
    "        similarity_scores[page] = similarity_scores[page] / (doc_lens[page] * query_len)\n",
    "\n",
    "    return similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e3f63c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize query text\n",
    "def tokenize_query(query_text):\n",
    "    text = query_text.lower()\n",
    "    text = re.sub(\"[^a-z]+\", \" \", text)\n",
    "    tokens = text.split()\n",
    "    clean_stem_tokens = [\n",
    "        st.stem(token)\n",
    "        for token in tokens\n",
    "        if (token not in stop_words and st.stem(token) not in stop_words)\n",
    "        and len(st.stem(token)) > 2\n",
    "    ]\n",
    "    return clean_stem_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd3d5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_relevant_pages(count, webpages):\n",
    "    for i in range(count, count + 10):\n",
    "        try:\n",
    "            url_no = int(webpages[i][0])\n",
    "\n",
    "        # executed when their are no more relevant pages to display\n",
    "        except Exception as e:\n",
    "            print(\"\\nNo more results found !!\")\n",
    "            break\n",
    "\n",
    "        if urls.get(url_no, None):\n",
    "            print(i + 1, urls.get(url_no))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c09877fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating IDF for all tokens\n",
    "webpages_idf = calc_idf(inverted_index)\n",
    "\n",
    "# finding the frequency of most frequent token in each webpage\n",
    "for page in webpages_tokens:\n",
    "    max_freq[page] = Counter(webpages_tokens[page]).most_common(1)[0][1]\n",
    "\n",
    "# calculating TF-IDF for all tokens\n",
    "webpages_tf_idf = calc_tfidf(inverted_index)\n",
    "\n",
    "# calculating document vector length for all webpages\n",
    "webpages_lens = doc_len_pages(webpages_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdc99606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     ---UIC Web Search Engine---\n",
      "\n",
      "Enter a search query: Ugo Buy\n",
      "\n",
      "\n",
      "1 https://advance.uic.edu/news-stories/home-buying-101\n",
      "2 https://advance.uic.edu/news-stories/mortgages-demystified\n",
      "3 https://cs.uic.edu/news-stories/engineering-students-learn-autonomous-vehicles-in-new-multidisciplinary-course\n",
      "4 https://engineering.uic.edu/news-stories/engineering-students-take-on-autonomous-vehicles-in-new-multidisciplinary-course-2\n",
      "5 https://law.uic.edu/student-support/its/computer-buying-guide\n",
      "6 https://cs.uic.edu/news-stories/fixing-online-privacy\n",
      "7 https://advance.uic.edu/news-stories/did-shakespeare-really-write-those-plays-lets-discuss-and-laugh\n",
      "8 https://today.uic.edu/phishing-scam-alert-gift-card-email-scam-targeting-uic-faculty-and-staff\n",
      "9 https://dining.uic.edu/about-us/faq\n",
      "10 https://library.uic.edu/help/article/1916/get-textbooks\n",
      "\n",
      "Do you want to more web page results? yes\n",
      "11 https://ask.library.uic.edu/faq/345794\n",
      "12 https://engineering.uic.edu/news-stories/dusable-scholars-program-wraps-up-first-year-with-100000-donation\n",
      "13 https://today.uic.edu/student-run-contract-tracing-program-highly-effective-in-curbing-covid-19-transmission-report-shows\n",
      "14 https://dining.uic.edu/about-us/ozzi\n",
      "15 https://publichealth.uic.edu/news-stories/new-research-highlights-successes-of-sphs-contact-tracing-for-uic\n",
      "16 https://sustainability.uic.edu/green-campus/recycling/sustainable-purchasing\n",
      "17 https://library.uic.edu/about/policies/code-of-conduct\n",
      "18 https://ahs.uic.edu/applying/tuition-and-aid\n",
      "19 https://engineering.uic.edu/news-stories/september-career-fair-helps-kick-off-employer-recruitment\n",
      "20 https://police.uic.edu/services/safe-exchange-zone\n",
      "\n",
      "Do you want to more web page results? no\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n                     ---UIC Web Search Engine---\\n\")\n",
    "# take input query from user\n",
    "query = str(input(\"Enter a search query: \"))\n",
    "print(\"\\n\")\n",
    "# tokenize input query\n",
    "query_tokens = tokenize_query(query)\n",
    "\n",
    "# calculate cosine similarity scores\n",
    "query_sim_pages = calc_cos_sim_scores(query_tokens, webpages_lens)\n",
    "\n",
    "# sort cosine similarities in descending order\n",
    "most_relevant_pages = sorted(query_sim_pages.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "yes = {\"y\", \"yes\"}\n",
    "# to implement do-while loop\n",
    "first_pass = True\n",
    "count = 0\n",
    "\n",
    "while first_pass or choice.lower() in yes:\n",
    "    first_pass = False\n",
    "    show_relevant_pages(count, most_relevant_pages)\n",
    "    choice = str(input(\"\\nDo you want to more web page results? \"))\n",
    "    count += 10\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
